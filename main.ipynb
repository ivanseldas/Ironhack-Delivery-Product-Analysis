{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_courier = pd.read_excel('./project_dataset/sql_tables_sample/customer_courier_chat_messages.xlsx')\n",
    "orders = pd.read_excel('./project_dataset/sql_tables_sample/orders.xlsx')\n",
    "fake = pd.read_excel('./project_dataset/python_raw_data/fake_orders_test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['order_id', 'activation_time_local', 'country_code', 'store_address',\n",
       "       'final_status', 'payment_status', 'products', 'products_total',\n",
       "       'purchase_total_price', 'pedidos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.rename(columns={'activation_time_local: ': 'activation_time_local', 'products: ': 'products', 'products_total: ':'products_total'}, inplace=True)\n",
    "fake.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create a Spark dataframe from Pandas dataframe\n",
    "messages = spark.createDataFrame(customer_courier)\n",
    "messages.createOrReplaceTempView(\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------+--------+-----------------------+--------+----------------+----------+-----------------+\n",
      "| sender_app_type|customer_id| from_id|   to_id|chat_started_by_message|order_id|     order_stage|courier_id|message_sent_time|\n",
      "+----------------+-----------+--------+--------+-----------------------+--------+----------------+----------+-----------------+\n",
      "|    Customer iOS|   17071099|17071099|16293039|                  false|59528555|      PICKING_UP|  16293039| 19/08/2019 08:03|\n",
      "|     Courier iOS|   17071099|16293039|17071099|                  false|59528555|        ARRIVING|  16293039| 19/08/2019 08:01|\n",
      "|    Customer iOS|   17071099|17071099|16293039|                  false|59528555|      PICKING_UP|  16293039| 19/08/2019 08:00|\n",
      "| Courier Android|   12874122|18325287|12874122|                   true|59528038|ADDRESS_DELIVERY|  18325287| 19/08/2019 07:59|\n",
      "|Customer Android|   13289990|13289990|18019844|                  false|59528038|ADDRESS_DELIVERY|  18019844| 25/12/2021 17:45|\n",
      "+----------------+-----------+--------+--------+-----------------------+--------+----------------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows\n",
    "messages.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "WITH message_times AS (\n",
    "    SELECT \n",
    "        order_id,\n",
    "        -- Time of the first message from the customer\n",
    "        MIN(CASE WHEN from_id = customer_id THEN message_sent_time END) AS first_customer_message_time,\n",
    "        \n",
    "        -- Time of the first message from the courier\n",
    "        MIN(CASE WHEN from_id = courier_id THEN message_sent_time END) AS first_courier_message_time,\n",
    "        \n",
    "        -- Number of messages from the customer\n",
    "        SUM(CASE WHEN from_id = customer_id THEN 1 ELSE 0 END) AS customer_message_count,\n",
    "        \n",
    "        -- Number of messages from the courier\n",
    "        SUM(CASE WHEN from_id = courier_id THEN 1 ELSE 0 END) AS courier_message_count,\n",
    "        \n",
    "        -- First sender of the message\n",
    "        CASE \n",
    "            WHEN MIN(CASE WHEN from_id = customer_id THEN message_sent_time END) < \n",
    "                 MIN(CASE WHEN from_id = courier_id THEN message_sent_time END) \n",
    "            THEN 'customer' \n",
    "            ELSE 'courier' \n",
    "        END AS first_sender,\n",
    "        \n",
    "        -- Time of the first message in the conversation\n",
    "        MIN(message_sent_time) AS first_message_time,\n",
    "        \n",
    "        -- Time of the last message in the conversation\n",
    "        MAX(message_sent_time) AS last_message_time\n",
    "    FROM messages\n",
    "    GROUP BY order_id\n",
    "),\n",
    "response_time AS (\n",
    "    SELECT \n",
    "        m1.order_id,\n",
    "        TIMESTAMPDIFF(\n",
    "            SECOND,\n",
    "            MIN(m1.message_sent_time),\n",
    "            MIN(m2.message_sent_time)\n",
    "        ) AS reaction_time\n",
    "    FROM messages m1\n",
    "    JOIN messages m2 ON m1.order_id = m2.order_id\n",
    "                     AND m2.message_sent_time > m1.message_sent_time\n",
    "                     AND m2.from_id != m1.from_id\n",
    "    GROUP BY m1.order_id\n",
    "),\n",
    "last_stage AS (\n",
    "    SELECT \n",
    "        order_id,\n",
    "        order_stage AS last_message_stage\n",
    "    FROM (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            order_stage,\n",
    "            ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY message_sent_time DESC) AS rn\n",
    "        FROM messages\n",
    "    ) AS last_msg\n",
    "    WHERE rn = 1\n",
    ")\n",
    "SELECT \n",
    "    mt.order_id,\n",
    "    mt.first_customer_message_time,\n",
    "    mt.first_courier_message_time,\n",
    "    mt.customer_message_count,\n",
    "    mt.courier_message_count,\n",
    "    mt.first_sender,\n",
    "    mt.first_message_time,\n",
    "    mt.last_message_time,\n",
    "    ls.last_message_stage\n",
    "FROM message_times mt\n",
    "JOIN response_time rt ON mt.order_id = rt.order_id\n",
    "JOIN last_stage ls ON mt.order_id = ls.order_id\n",
    "\"\"\")\n",
    "\n",
    "# Save result as a table if needed\n",
    "# result.write.saveAsTable(\"customer_courier_conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------+--------------------------+----------------------+---------------------+------------+------------------+-----------------+------------------+\n",
      "|order_id|first_customer_message_time|first_courier_message_time|customer_message_count|courier_message_count|first_sender|first_message_time|last_message_time|last_message_stage|\n",
      "+--------+---------------------------+--------------------------+----------------------+---------------------+------------+------------------+-----------------+------------------+\n",
      "|59528111|           02/07/2021 13:25|          02/11/2019 06:14|                     3|                    3|    customer|  02/07/2021 13:25| 22/06/2020 21:57|          ARRIVING|\n",
      "|59528214|           11/12/2019 01:33|          16/08/2021 00:23|                     2|                    1|    customer|  11/12/2019 01:33| 22/03/2021 23:10|          ARRIVING|\n",
      "|59528420|                       NULL|          22/04/2019 05:06|                     0|                    2|     courier|  22/04/2019 05:06| 23/05/2021 19:13|  ADDRESS_DELIVERY|\n",
      "|59528250|           16/12/2019 19:25|          10/02/2019 12:15|                     2|                    2|     courier|  10/02/2019 12:15| 26/05/2019 01:39|          ARRIVING|\n",
      "|59528394|           08/09/2021 00:37|          18/07/2020 03:22|                     2|                    1|    customer|  08/09/2021 00:37| 18/07/2020 03:22|  ADDRESS_DELIVERY|\n",
      "|59528493|           10/11/2020 14:22|          06/08/2020 15:14|                     5|                    3|     courier|  06/08/2020 15:14| 30/01/2019 05:24|  ADDRESS_DELIVERY|\n",
      "|59528396|           04/04/2019 01:07|          06/01/2019 07:01|                     3|                    3|    customer|  04/04/2019 01:07| 27/06/2019 06:26|          ARRIVING|\n",
      "|59528134|           10/05/2021 07:49|          20/06/2019 09:12|                     2|                    2|    customer|  10/05/2021 07:49| 30/06/2019 00:00|        PICKING_UP|\n",
      "|59528532|           11/11/2021 02:00|          13/10/2021 16:25|                     1|                    3|    customer|  11/11/2021 02:00| 14/04/2021 12:15|          ARRIVING|\n",
      "|59528312|           05/06/2019 22:46|          23/09/2021 21:11|                     5|                    1|    customer|  05/06/2019 22:46| 23/09/2021 21:11|          ARRIVING|\n",
      "|59528422|           09/06/2020 04:33|                      NULL|                     2|                    0|     courier|  09/06/2020 04:33| 17/04/2021 17:19|  ADDRESS_DELIVERY|\n",
      "|59528139|           23/11/2019 00:49|          02/11/2021 16:16|                     2|                    6|     courier|  02/11/2021 16:16| 29/08/2021 14:56|  ADDRESS_DELIVERY|\n",
      "|59528340|           22/04/2021 18:48|          24/03/2019 00:10|                     2|                    2|    customer|  22/04/2021 18:48| 27/09/2021 13:53|  ADDRESS_DELIVERY|\n",
      "|59528490|           05/09/2021 20:37|          04/08/2020 01:09|                     2|                    4|     courier|  04/08/2020 01:09| 22/04/2021 18:53|        PICKING_UP|\n",
      "|59528169|                       NULL|          10/12/2019 21:16|                     0|                    3|     courier|  10/12/2019 21:16| 31/01/2019 18:19|        PICKING_UP|\n",
      "+--------+---------------------------+--------------------------+----------------------+---------------------+------------+------------------+-----------------+------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Orders Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark dataframe from Pandas dataframe\n",
    "fake_orders = spark.createDataFrame(fake)\n",
    "fake_orders.createOrReplaceTempView(\"fake_orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Percentage of orders under-authorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.87%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_1 = \"\"\"\n",
    "SELECT \n",
    "    ROUND((SUM(CASE WHEN purchase_total_price < products_total THEN 1 ELSE 0 END) / COUNT(*)) * 100, 2) AS under_authorized_percentage\n",
    "FROM fake_orders;\n",
    "\"\"\"\n",
    "response_1 = spark.sql(query_1).collect()\n",
    "\n",
    "print(f\"{response_1[0][0]}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Percentage of orders with a 20% incremental authorization on the checkout amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.79%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"\"\"\n",
    "SELECT \n",
    "    ROUND((SUM(CASE WHEN purchase_total_price >= products_total * 1.2 THEN 1 ELSE 0 END) / COUNT(*)) * 100, 2) AS incremental_authorized_percentage\n",
    "FROM fake_orders;\n",
    "\"\"\"\n",
    "response_2 = spark.sql(query_2).collect()\n",
    "\n",
    "print(f\"{response_2[0][0]}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Differences by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Country: UA, Under-authorized Percentage: 25.71%, Incremental Authorized Percentage: 26.68%\n",
      "Country: RO, Under-authorized Percentage: 27.5%, Incremental Authorized Percentage: 27.5%\n",
      "Country: PT, Under-authorized Percentage: 33.55%, Incremental Authorized Percentage: 35.64%\n",
      "Country: CL, Under-authorized Percentage: 12.95%, Incremental Authorized Percentage: 87.05%\n",
      "Country: GT, Under-authorized Percentage: 19.58%, Incremental Authorized Percentage: 20.11%\n",
      "Country: ES, Under-authorized Percentage: 27.16%, Incremental Authorized Percentage: 29.81%\n",
      "Country: EC, Under-authorized Percentage: 26.71%, Incremental Authorized Percentage: 36.28%\n",
      "Country: TR, Under-authorized Percentage: 23.19%, Incremental Authorized Percentage: 31.58%\n",
      "Country: CR, Under-authorized Percentage: 33.15%, Incremental Authorized Percentage: 24.03%\n",
      "Country: FR, Under-authorized Percentage: 28.04%, Incremental Authorized Percentage: 33.43%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_3 = \"\"\"\n",
    "SELECT \n",
    "    country_code,\n",
    "    ROUND((SUM(CASE WHEN purchase_total_price < products_total THEN 1 ELSE 0 END) / COUNT(*)) * 100, 2) AS under_authorized_percentage,\n",
    "    ROUND((SUM(CASE WHEN purchase_total_price >= products_total * 1.2 THEN 1 ELSE 0 END) / COUNT(*)) * 100, 2) AS incremental_authorized_percentage\n",
    "FROM fake_orders\n",
    "GROUP BY country_code;\n",
    "\"\"\"\n",
    "response_3 = spark.sql(query_3).collect()\n",
    "\n",
    "print(\"Answer:\")\n",
    "\n",
    "for row in response_3[:10]:\n",
    "    print(f\"Country: {row[0]}, Under-authorized Percentage: {row[1]}%, Incremental Authorized Percentage: {row[2]}%\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Values necessary to capture the amount for the remaining orders that would fall outside of incremental authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Order ID: 33512615, Amount needed for capture: 0.7519999999999998\n",
      "Order ID: 33512451, Amount needed for capture: 5720000000000000.0\n",
      "Order ID: 33530892, Amount needed for capture: 15.54\n",
      "Order ID: 33512273, Amount needed for capture: 0.2959999999999998\n",
      "Order ID: 33524023, Amount needed for capture: 10.618\n",
      "Order ID: 33511759, Amount needed for capture: 0.3939999999999997\n",
      "Order ID: 33511416, Amount needed for capture: 0.786\n",
      "Order ID: 33557565, Amount needed for capture: 3.1079999999999997\n",
      "Order ID: 33557508, Amount needed for capture: 6.875999999999988e+16\n",
      "Order ID: 33510806, Amount needed for capture: 0.8839999999999995\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_4 = \"\"\"\n",
    "SELECT \n",
    "    order_id, \n",
    "    (products_total * 1.2) - purchase_total_price AS amount_needed_for_capture\n",
    "FROM fake_orders\n",
    "WHERE purchase_total_price < products_total * 1.2;\n",
    "\"\"\"\n",
    "response_4 = spark.sql(query_4).collect()\n",
    "\n",
    "print(\"Answer:\")\n",
    "\n",
    "for row in response_4[:10]:\n",
    "    print(f\"Order ID: {row[0]}, Amount needed for capture: {row[1]}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Most problematic stores in terms of order quantity and monetary value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most problematic stores in terms of order quantity\n",
      "Store: 28671, Number of orders: 492\n",
      "Store: 12513, Number of orders: 276\n",
      "Store: 14455, Number of orders: 252\n",
      "Store: 28675, Number of orders: 248\n",
      "Store: 28712, Number of orders: 242\n",
      "Store: 28286, Number of orders: 214\n",
      "Store: 28669, Number of orders: 201\n",
      "Store: 50832, Number of orders: 197\n",
      "Store: 67084, Number of orders: 183\n",
      "Store: 51363, Number of orders: 179\n",
      "\n",
      "Most problematic stores in terms of monetary value\n",
      "Store: 28286, Total value: 1.6295199999999987e+18\n",
      "Store: 51363, Total value: 8.606700000000001e+17\n",
      "Store: 62935, Total value: 8.533269999999997e+17\n",
      "Store: 28285, Total value: 8.182599999999992e+17\n",
      "Store: 66607, Total value: 7.767499999999999e+17\n",
      "Store: 51374, Total value: 7.354099999999995e+17\n",
      "Store: 11361, Total value: 7.333699999999999e+17\n",
      "Store: 14455, Total value: 7.29140000000002e+17\n",
      "Store: 11694, Total value: 6.307900000000003e+17\n",
      "Store: 36698, Total value: 6.2369e+17\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query_5a = \"\"\"\n",
    "SELECT \n",
    "    store_address,\n",
    "    COUNT(order_id) AS number_of_orders\n",
    "FROM fake_orders\n",
    "GROUP BY store_address\n",
    "ORDER BY number_of_orders DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "response_5a = spark.sql(query_5a).collect()\n",
    "\n",
    "print(\"Most problematic stores in terms of order quantity\")\n",
    "for row in response_5a[:10]:\n",
    "    print(f\"Store: {row[0]}, Number of orders: {row[1]}\")\n",
    "\n",
    "query_5b = \"\"\"\n",
    "SELECT \n",
    "    store_address,\n",
    "    SUM(purchase_total_price) AS total_value\n",
    "FROM fake_orders\n",
    "GROUP BY store_address\n",
    "ORDER BY total_value DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "response_5b = spark.sql(query_5b).collect()\n",
    "\n",
    "print(\"\\nMost problematic stores in terms of monetary value\")\n",
    "\n",
    "for row in response_5b[:10]:\n",
    "    print(f\"Store: {row[0]}, Total value: {row[1]}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation between price difference and order cancellation for under-authorized orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Average price difference: 8.51, Cancellation probability: 0.0\n",
      "Average price difference: 0.8599999999999994, Cancellation probability: 0.0\n",
      "Average price difference: 0.09999999999999964, Cancellation probability: 0.0\n",
      "Average price difference: 6909999999999979.0, Cancellation probability: 0.0\n",
      "Average price difference: 4.4799999999999995, Cancellation probability: 0.0\n",
      "Average price difference: 1.7599999999999998, Cancellation probability: 0.0\n",
      "Average price difference: 7209999999999981.0, Cancellation probability: 0.0\n",
      "Average price difference: 19.27, Cancellation probability: 0.0\n",
      "Average price difference: 5.86, Cancellation probability: 0.0\n",
      "Average price difference: 8959999999999991.0, Cancellation probability: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query_6 = \"\"\"\n",
    "SELECT \n",
    "    AVG(products_total - purchase_total_price) AS avg_price_difference,\n",
    "    AVG(CASE WHEN final_status = 'cancelled' THEN 1 ELSE 0 END) AS cancellation_probability\n",
    "FROM fake_orders\n",
    "WHERE purchase_total_price < products_total\n",
    "GROUP BY (products_total - purchase_total_price);\n",
    "\"\"\"\n",
    "response_6 = spark.sql(query_6).collect()\n",
    "\n",
    "print(\"Answer:\")\n",
    "\n",
    "for row in response_6[:10]:\n",
    "    print(f\"Average price difference: {row[0]}, Cancellation probability: {row[1]}\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
